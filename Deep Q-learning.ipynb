{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5344ebab-14e5-4dcd-9604-b9bf8fb31cb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Q-learning - Floating Maze\n",
    "##### Max Brynolf (max.brynolf@hotmail.com)\n",
    "The following code trains a deep reinforcement learning model to play a maze game coded in Java. The game has to be launched prior to running the code in this document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25199d-23a1-420f-a84f-8f5e8cfed67c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dcf574-7a1a-4005-b5b5-397871dad9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Connection to Java\n",
    "from py4j.java_gateway import JavaGateway\n",
    "\n",
    "# Neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509a1b0-7fbc-43aa-a9e5-0189b6d2d0fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to the Java Application\n",
    "\n",
    "The mainProcess-variable will represent the main class `Maze.java`. Before running this code, launch `Maze.java` with `TrainingMode.RL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce100e6b-d441-4738-a486-b65b6b855d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gateway = JavaGateway()\n",
    "mainProcess = gateway.entry_point # mainProcess is the Maze class\n",
    "mainProcess.startWindow(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ae6a0-c18a-4304-b359-c68263eb4f5a",
   "metadata": {},
   "source": [
    "### Action-Value Neural Network\n",
    "The action-value function $Q(s_t, a_t)$, approximating $\\mathbb{E}_{\\pi}[G_t | s_t, a_t]$, is approximated by a neural network. The network structure is defined below. Note that the network outputs all possible actions $f(s) = [Q(s, a_1), Q(s, a_2), \\dots , Q(s, a_n)]$, such that $Q(s, a_i) = f_i (s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f39d3-ec94-4037-9ce6-9671fe0333dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActionValueNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, screen_size, res, state_size, num_of_actions):\n",
    "        super(ActionValueNetwork, self).__init__()\n",
    "\n",
    "        conv_output_size = 32\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.conv_layers.append(nn.Conv2d(state_size, 16, kernel_size = 9, stride = 4, padding = 4, bias = True))\n",
    "        self.conv_layers.append(nn.Conv2d(16, conv_output_size, kernel_size = 5, stride = 2, padding = 2, bias = True))\n",
    "        \n",
    "        # Calculate the input dimension to the FC-layer\n",
    "        output_dim = round(screen_size / res)\n",
    "        for conv_layer in self.conv_layers:\n",
    "            K = conv_layer.kernel_size[0]\n",
    "            P = conv_layer.padding[0]\n",
    "            S = conv_layer.stride[0]\n",
    "            output_dim = math.floor((output_dim - K + 2*P)/S) + 1\n",
    "        self.fc = nn.Linear(output_dim**2 * conv_output_size, num_of_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for res_layer in self.conv_layers:\n",
    "            y = F.relu(res_layer(y))\n",
    "        y = torch.flatten(y, start_dim = 1)\n",
    "        y = self.fc(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de947d5f-2a7c-4444-aa9f-a8b129740740",
   "metadata": {},
   "source": [
    "### Actions, States and Rewards\n",
    "The following function converts the raw screen data sent from the Java program to a PyTorch tensor with the correct shape. The Java method `ScreenData.getAllPixels` provides a `byte[]`-array in the format:\n",
    "$$\n",
    "[K_{11}, K_{12}, K_{13}, \\dots , K_{1M}, K_{21}, K_{22}, \\dots , K_{MN}]\n",
    "$$\n",
    "where $K_{ij}$ is the grayscale decimal values for the pixel at position $(i, j)$. The method `ScreenData.getAllPixels` also includes a `resolution` parameter, which downsamples each dimension of the pixel data by a factor of `resolution`, before being converted to the vector above. For example, having a screen size of $300$ and setting `resolution = 2` returns pixel data of size $150 \\cdot 150$. The output-tensor returned by the function has the same structure as the array above, but reshaped into a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881bc1a-b97b-4f8f-a7b2-b1b2fe88aefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tensor_from_screen_data(data):\n",
    "    arr = np.frombuffer(data, dtype=np.uint8).astype(np.float32) / 255\n",
    "    tens = torch.from_numpy(arr)\n",
    "    screen_dim = math.floor(math.sqrt(len(data))) # this holds for width = height, which is assumed throughout\n",
    "    return tens.view(1, screen_dim, screen_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf57e7d-7953-47f5-ab2c-72332737f9a9",
   "metadata": {},
   "source": [
    "The following functions allow the agent to take actions in the game and return the relevant data.\n",
    "- `make_action(a, r, s, d)`: The agent takes action `a` for `d - 1` frames and stops the action for `1` frame. This makes sure that only one key is pressed at the same time. In return, it receives a `byte[]`-array of the form:\n",
    "$$\n",
    "[R, T, K_1, K_2, K_3, \\dots , K_{n}]\n",
    "$$\n",
    "where $[K_i]$ contains the grayscale pixel data, with each screen dimension scaled by a factor of `1/r`, $R$ is the reward and $T$ is a variable indicating whether the episode has terminated or not. In order to calculate the reward from the current game score, the old score must be included in `s`. This makes $R$ the change in game score, i.e. the difference between the current game score and the game score at the last iteration, hence $R_t = s_t - s_{t - 1}$. The actions `a` correspond to different key presses along with a release key action:\n",
    "$$\n",
    "a \\in \\mathcal{A} = \\{\\leftarrow, \\rightarrow, \\uparrow, \\downarrow, \\varnothing \\}\n",
    "$$\n",
    "Note that `d` cannot be lower than 2, since 1 frame is reserved for releasing the key. This behavior can of course be modified, but since a human can beat the game with way less frame-perfect control, using a single frame to only release keys can easily be justified.\n",
    "- `make_action_realtime(a, r, t, d)`: Calls `make_action(a, r, 0, d)` with a delay `t` added such that subsequent calls to the function gives a viewer-friendly episode. This function can be called when testing the model. The old score is set to 0 since this doesn't matter when the model is not being trained.\n",
    "- `reset()`: Resets the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ef061-315f-4846-9b15-f99022a5a07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_action(action_type, screen_res, old_score = 0, frame_skip = 2):\n",
    "    if (type(action_type) is np.int64):\n",
    "        action_type = action_type.item()\n",
    "    data = \"\"\n",
    "    states = []\n",
    "    \n",
    "    if action_type == 4: # key released\n",
    "        for i in range(frame_skip):\n",
    "            data = mainProcess.stepWindowTraining(screen_res)\n",
    "            states.append(data[2:])\n",
    "            if data[1] == 1: # if episode terminates within frame skip\n",
    "                break\n",
    "    else: #key pressed\n",
    "        for i in range(frame_skip):\n",
    "            data = mainProcess.stepWindowTraining(action_type, i < frame_skip - 1, screen_res)\n",
    "            states.append(data[2:])\n",
    "            if data[1] == 1: # if episode terminates within frame skip\n",
    "                break # if it breaks here, the new frames will not be used anyways, so it does not matter\n",
    "    \n",
    "    total_score = data[0]\n",
    "    reward = total_score - old_score\n",
    "    done = data[1]\n",
    "    return states, reward, done, total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204ceb3-c8ae-476c-8c15-7249b9b8d2a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_action_realtime(action_type, screen_res, interval, frame_skip = 1):\n",
    "    states, _, done, _ = make_action(action_type, screen_res, 0, frame_skip)\n",
    "    time.sleep(interval)\n",
    "    return states, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82e77d-a099-4f90-a070-adf9feaa8969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reset():\n",
    "    mainProcess.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517833e-1238-4fd4-84d8-6318a53b470d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c63a38-c0f0-4828-ad45-531a8c7053dc",
   "metadata": {},
   "source": [
    "To train the model, a set of parameters are needed. Their descriptions are included as comments to the right of each parameter. Some elaborated comments are needed for a few parameters, included below:\n",
    "\n",
    "- `screen_dim`: Throughout the code, the window is always assumed to be a square. If this is to be changed, make sure to modify the code accordingly.\n",
    "- `screen_res`: Note that increasing this will speed up the communication between Java and Python, but will reduce the resolution of the image seen by the neural network.\n",
    "- `state_size`: For each step, `frame_skip` frames are appended to the current state. The state holds `state_size` frames and removes the oldest frames when it is full. Hence, if `frame_skip` isn't equal to `state_size`, it is possible that some frames will not be used, even if the code still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325a867-bc6f-4d36-8cdb-82386af55fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "screen_dim = mainProcess.getConstant(2) # screen dimension of the game\n",
    "num_of_actions = 5; # number of possible actions you can take in the game\n",
    "screen_res = 4 # defines how much the game is scaled down\n",
    "timestep_interval = mainProcess.getConstant(4) # the timestep interval to be used during testing\n",
    "\n",
    "# Step parameters\n",
    "total_episodes = 10000 # number of episodes\n",
    "target_nn_update_steps = 10000 # number of steps until the target network weights are updated\n",
    "main_nn_update_steps = 100 # number of steps until the main network weights are updated\n",
    "state_size = 4 # the size of the agent state, equivalent to the number of frames that are sent into the network\n",
    "frame_skip = 4 # the number of frames before the state is updated, during which the same action is executed\n",
    "max_steps = 1000 # the maximum number of steps before the episode is automatically terminated\n",
    "\n",
    "# Exploration-exploitation trade-off\n",
    "fixed_eps_episodes = 1000 # episodes at the start of training, in which exploration is set to eps_fixed\n",
    "eps_fixed = 1 # the fixed value of eps, held for fixed_eps_episodes at the start of training\n",
    "eps_max = 1 # maximum value of the exploration constant\n",
    "eps_min = 0.1 # minimum value of the exploration constant\n",
    "eps_decay_rate = 1.5 / (total_episodes - fixed_eps_episodes) # decay rate for the exploration constant\n",
    "discount_factor = 0.99 # factor that specifies how important immediate reward is as opposed to long-term reward\n",
    "\n",
    "# Other training parameters\n",
    "replay_mem_min = 250 # the minimum size of the replay memory, for the neural network to be trained\n",
    "replay_mem_length = 10000 # the size of the replay memory\n",
    "adam_lr = 1e-3 # the learning rate used by the Adam optimizer within the gradient descent algorithm\n",
    "batch_size = 64 # the batch size used when training the neural network\n",
    "device = torch.device(\"cpu\") # the device used during training\n",
    "graph_update_frequency = 50 # number of episodes until the graphs are updated\n",
    "\n",
    "# Print time-dependent parameters\n",
    "episode_list = np.arange(total_episodes)\n",
    "eps_list = [eps_fixed] * fixed_eps_episodes\n",
    "eps_list += (eps_min + (eps_max - eps_min) * np.exp(-eps_decay_rate * (episode_list[fixed_eps_episodes:] - fixed_eps_episodes))).tolist()\n",
    "plt.plot(episode_list, eps_list)\n",
    "plt.title(\"Time-dependent parameters\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Exploration constant\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e1754-d6d0-4fca-a096-19223bb51f3c",
   "metadata": {},
   "source": [
    "The following function fits the neural network to the data in the replay memory. Given a memory `mem`, it randomly selects a batch of `batch_size` samples and updates the networks weights accordingly. The basis for the update is the Bellman Optimality Equation, relating the value of the optimal state-action-value function in the current state to its value in the next:\n",
    "$$\n",
    "Q^*(s, a) = r(s, a) + \\gamma \\sum_{s'} p(s' | s, a) \\max_{a' \\in \\mathcal{A}} Q^*(s', a')\n",
    "$$\n",
    "Considering a transition from $s, a$ to $s'$, and letting the expectation $\\sum_{s'} p(s' | s, a) \\max_{a' \\in \\mathcal{A}} Q^*(s', a')$ be replaced with the sample $ \\max_{a' \\in \\mathcal{A}} Q^*(s', a')$, we can use $r(s, a) + \\gamma \\max_{a' \\in \\mathcal{A}} Q(s', a')$ as a target when training our network. Note that $Q$ is bootstrapped in place of $Q^*$, since we cannot know the real $Q^*$. This can be shown to converge towards $Q^*$, under the right circumstances. In other words, the idea is to fit the neural network with respect to the one-step TD error $\\delta$, defined as:\n",
    "$$\n",
    "\\delta = Q(s, a) - \\left(r(s, a) + \\gamma \\max_{a' \\in \\mathcal{A}} Q(s', a')\\right)\n",
    "$$\n",
    "Since deep Q-learning is employed, $Q$ is approximated by $Q_\\mathbf{w}$, where $\\mathbf{w}$ is a vector with the neural network weights. The weights are updated in the following steps:\n",
    "1. A batch is sampled from the replay memory, and the states $S_t$ along with their successive states $S_{t+1}$ are extracted.\n",
    "2. The current approximation $Q(s_t, a_t)$ is calculated for each transition in the batch\n",
    "3. The current approximation of the Q-values at $S_{t+1}$ are calculated using the target network, for each transition in the batch.\n",
    "4. The target values Q-values $Y_t$ are calculated according to the Bellman equation $Y_t = r(s_t, a_t) + \\gamma \\max_a Q(s_{t+1}, a)$\n",
    "5. The network weights are updated with respect to the loss between $Q(s_t, a_t)$ and $r(s_t, a_t) + \\gamma \\max_a Q(s_{t+1}, a)$. This is done using Huber loss, defined by:\n",
    "$$\n",
    "J(\\delta) = \\begin{cases}\n",
    "\\frac{1}{2}\\delta^2, & \\text{for } \\left| \\delta \\right| \\leq 1\\\\\n",
    "\\delta - \\frac{1}{2} & \\text{for } \\left| \\delta \\right| > 1\n",
    "\\end{cases}$$\n",
    "6. The loss is calculated and added to `loss_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb19f7-b427-4950-a00b-3281fc1f3b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(mem, main_nn, target_nn, optimizer, batch_size, device, loss_data):\n",
    "    \n",
    "    # Assure that replay memory is large enough\n",
    "    if (len(mem) < batch_size):\n",
    "        return\n",
    "    \n",
    "    # Sample a batch and extract states\n",
    "    mini_batch = random.sample(mem, batch_size)\n",
    "    states = np.array([transition[0] for transition in mini_batch])\n",
    "    new_states = np.array([transition[3] for transition in mini_batch])\n",
    "    \n",
    "    # Calculate Q of current states and of the next states\n",
    "    state_vals = main_nn(torch.from_numpy(states).to(device))\n",
    "    next_state_vals = target_nn(torch.from_numpy(new_states).to(device)).detach().cpu().squeeze().numpy()\n",
    "    \n",
    "    # Create vectors with states and targets, based on the Bellman equation\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, (S, action, reward, S_new, done) in enumerate(mini_batch):\n",
    "        if done == 0:\n",
    "            target = reward + discount_factor * np.max(next_state_vals[index])\n",
    "        else:\n",
    "            target = reward\n",
    "        X.append(state_vals[index][action].reshape(1))\n",
    "        Y.append(target)\n",
    "    \n",
    "    # Update the network weights with gradient descent\n",
    "    X = torch.cat(X).to(device)\n",
    "    Y = torch.Tensor(Y).to(device = device, dtype = torch.float32)\n",
    "    loss_function = nn.SmoothL1Loss()\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(X, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Append loss data\n",
    "    loss_data.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a65d4-5385-4947-b062-a9d716ccb624",
   "metadata": {},
   "source": [
    "The following functions are defined below:\n",
    "- `routine(target_nn, rewards)`: Consists of the main training loop. It updates the user during the training process, while training the network `main_nn` and filling the `rewards`-list with rewards.\n",
    "- `test_model(model, eps, episodes)`: Simulates `episodes` episodes in real-time using model `model`, with constant exploration set to `eps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19907a-25dc-4bf6-bbd1-5050005da025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def routine(main_nn, rewards):\n",
    "    try:\n",
    "        \n",
    "        # Initialize neural networks\n",
    "        target_nn = copy.deepcopy(main_nn)\n",
    "        main_nn.to(device)\n",
    "        target_nn.to(device)\n",
    "        optimizer = optim.Adam(main_nn.parameters(), lr = adam_lr)\n",
    "        \n",
    "        # Initialize plot\n",
    "        ma_lengths = [100, 1000] # moving average lengths\n",
    "        x = []\n",
    "        y = []\n",
    "        loss_data = [[],[]]\n",
    "        \n",
    "        # Reset environment\n",
    "        reset()\n",
    "        episode = 0\n",
    "        mem = deque(maxlen = replay_mem_length) # replay memory\n",
    "        target_update_counter = 0 # keeps track of when to update the target network\n",
    "        main_update_counter = 0 # keeps track of when to update the main network\n",
    "\n",
    "        # Loop through episodes\n",
    "        for _ in range(total_episodes):\n",
    "            \n",
    "            # Reset or update episode-parameters\n",
    "            done = 0\n",
    "            score = 0\n",
    "            state = deque(maxlen = state_size)\n",
    "            used_steps = 0\n",
    "            episode_reward = 0\n",
    "            if episode < fixed_eps_episodes:\n",
    "                eps = eps_fixed\n",
    "            else:\n",
    "                eps = eps_min + (eps_max - eps_min) * np.exp(-eps_decay_rate * (episode - fixed_eps_episodes))\n",
    "            starting_frame = mainProcess.getPixelData(screen_res)\n",
    "            for i in range(state_size):\n",
    "                state.append(create_tensor_from_screen_data(starting_frame)) # starting state\n",
    "            \n",
    "            # Update graphs\n",
    "            if (episode % graph_update_frequency == 0):\n",
    "                update_graph(rewards, ma_lengths, loss_data)\n",
    "                print(\"Episode: {0} out of {1} ({2}% finished)\".format(episode, total_episodes, 100 * episode/total_episodes))\n",
    "                print(\"Eps: {0}\".format(eps))\n",
    "                print(\"Replay memory size: {0}/{1} ({2}%)\".format(len(mem), replay_mem_length, 100 * len(mem)/replay_mem_length))\n",
    "            \n",
    "            # Loop through steps\n",
    "            while done == 0:\n",
    "                \n",
    "                # Reset or update parameters\n",
    "                target_update_counter += 1\n",
    "                main_update_counter += 1\n",
    "                action = 0\n",
    "                used_steps += 1\n",
    "                \n",
    "                # Decide whether to explore or exploit, and which action to take\n",
    "                if np.random.uniform(0, 1) < eps:\n",
    "                    action = np.random.randint(0, num_of_actions - 1)\n",
    "                else:\n",
    "                    input_tensor = torch.cat(tuple(state)).unsqueeze(0).to(device)\n",
    "                    action = np.argmax(main_nn(input_tensor).detach().cpu().squeeze().numpy())\n",
    "                \n",
    "                # Take action and update state\n",
    "                frames, reward, done, score = make_action(action, screen_res, score, frame_skip)\n",
    "                new_state = state.copy()\n",
    "                for frame in frames:\n",
    "                    new_state.append(create_tensor_from_screen_data(frame))\n",
    "                if (used_steps > max_steps):\n",
    "                    reset()\n",
    "                    done = 1\n",
    "                \n",
    "                # Update replay memory\n",
    "                if len(state) == state_size:\n",
    "                    numpy_state = torch.cat(tuple(state)).detach().numpy()\n",
    "                    numpy_new_state = torch.cat(tuple(new_state)).detach().numpy()\n",
    "                    mem.append([numpy_state, action, reward, numpy_new_state, done])\n",
    "                \n",
    "                # Decide whether to update the main network\n",
    "                if main_update_counter > main_nn_update_steps:\n",
    "                    if len(mem) >= replay_mem_min:\n",
    "                        loss_data[0].append(episode) # append the x-value for the loss-graph\n",
    "                        train(mem, main_nn, target_nn, optimizer, batch_size, device, loss_data[1])\n",
    "                    main_update_counter = 0\n",
    "                \n",
    "                # Decide whether to update the target network\n",
    "                if target_update_counter >= target_nn_update_steps:\n",
    "                    target_nn.load_state_dict(main_nn.state_dict())\n",
    "                    target_update_counter = 0\n",
    "                \n",
    "                # Advance state and add reward to total episode reward\n",
    "                state = new_state\n",
    "                episode_reward += reward\n",
    "                \n",
    "            # Update episode-parameters add reward to list\n",
    "            episode += 1\n",
    "            rewards.append(episode_reward)\n",
    "            \n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted at episode {0}\".format(episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f67262b-137b-482d-ba48-43f2e3f1eb01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(model, eps, episodes):\n",
    "    try:\n",
    "        \n",
    "        # Reset environment\n",
    "        model.to(device)\n",
    "        reset()\n",
    "\n",
    "        # Loop through episodes\n",
    "        for i in range(episodes):\n",
    "            \n",
    "            # Reset or update episode-parameters\n",
    "            done = 0\n",
    "            state = deque(maxlen = state_size)\n",
    "            used_steps = 0\n",
    "            starting_frame = mainProcess.getPixelData(screen_res)\n",
    "            for i in range(state_size):\n",
    "                state.append(create_tensor_from_screen_data(starting_frame)) # starting state\n",
    "            \n",
    "            # Loop through steps\n",
    "            while done == 0:\n",
    "\n",
    "                # Update parameters\n",
    "                used_steps += 1\n",
    "\n",
    "                # Decide whether to explore or exploit, and which action to take\n",
    "                if np.random.uniform(0, 1) < eps:\n",
    "                    action = np.random.randint(0, num_of_actions - 1)\n",
    "                else:\n",
    "                    input_tensor = torch.cat(tuple(state)).unsqueeze(0).to(device)\n",
    "                    action = np.argmax(model(input_tensor).detach().cpu().numpy())\n",
    "\n",
    "                # Take action and update state\n",
    "                frames, done = make_action_realtime(action, screen_res, timestep_interval/1e3, frame_skip) # Take action\n",
    "                for frame in frames: \n",
    "                    state.append(create_tensor_from_screen_data(frame))\n",
    "                if (used_steps > max_steps):\n",
    "                    reset()\n",
    "                    done = 1\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Testing interrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d0068-807f-46a2-8a31-bc8340e925ee",
   "metadata": {},
   "source": [
    "The following function updates the graphs displayed during training. The `rewards`-parameter contains the rewards to be displayed, the `ma_lengths`-parameter is a vector containing the moving averages to apply to the rewards, and the `loss_data` contains the training losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851cbbe-9a6b-477a-818b-af45566e0867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_graph(rewards, ma_lengths, loss_data):\n",
    "    \n",
    "    # Initialize subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1)\n",
    "    clear_output(wait = True)\n",
    "    y = []\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    if len(rewards) > 0:\n",
    "        for ma_length in ma_lengths:\n",
    "            if len(rewards) < ma_length: # edge case when there aren't enough samples\n",
    "                averaged_y = [sum(rewards) / ma_length] * len(rewards)\n",
    "            else:\n",
    "                averaged_y = [sum(rewards[:ma_length - 1]) / (ma_length - 1)] * (ma_length - 1) # edge case data\n",
    "                averaged_y += np.convolve(rewards, np.ones(ma_length)/ma_length, mode = \"valid\").tolist()\n",
    "            y.append(averaged_y)\n",
    "    \n",
    "    # Raw reward-output\n",
    "    x = np.arange(len(rewards))\n",
    "    ax1.plot(x, rewards)\n",
    "    ax1.set_title(\"Reward\")\n",
    "    ax1.set(xlabel = \"Episode\", ylabel = \"Reward\")\n",
    "    \n",
    "    # Moving average of rewards\n",
    "    for i, line in enumerate(y):\n",
    "        ax2.plot(x, line, label = \"Moving average (N = {})\".format(ma_lengths[i]))\n",
    "    ax2.set_title(\"Filtered reward\")\n",
    "    ax2.set(xlabel = \"Episode\", ylabel = \"Reward average\")\n",
    "    if len(y) > 0:\n",
    "        ax2.legend(loc = \"upper right\", fontsize = 8)\n",
    "    \n",
    "    # Loss data\n",
    "    ax3.plot(loss_data[0], loss_data[1])\n",
    "    ax3.set_title(\"Training loss\")\n",
    "    ax3.set(xlabel = \"Episode\", ylabel = \"Training loss (last epoch)\")\n",
    "    \n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5158f-a04a-4448-aa1c-7690a8d3a996",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0684924-c72a-489f-8316-38c3dd33e24d",
   "metadata": {},
   "source": [
    "The following code trains, evaluates and plots data of the Deep Q-learning process. Moreover, an option is given to save and load an existing model. Note that a model can be trained, saved, then trained again by passing a loaded model into `routine` rather than creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a5412-77bb-4feb-b1b6-9ee3515ce612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "r = []\n",
    "net = ActionValueNetwork(screen_dim, screen_res, state_size, num_of_actions)\n",
    "routine(net, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7384288-081a-4e83-b144-7a8744905ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "test_model(net, 0.05, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a04cb-9f71-4b51-994a-13d1a6bc5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(net.cpu().state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72005f0f-4358-4025-a60f-85b204757403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "net = ActionValueNetwork(screen_dim, screen_res, state_size, num_of_actions)\n",
    "net.load_state_dict(torch.load(\"model.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
